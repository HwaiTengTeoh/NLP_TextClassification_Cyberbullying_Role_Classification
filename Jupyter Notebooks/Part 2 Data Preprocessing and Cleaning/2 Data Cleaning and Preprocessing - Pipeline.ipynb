{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a1ce83",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#fff; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:15px 15px; color:#ff6f69; font-size:40px'>2. Data Cleaning and Preprocessing - Pipeline </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44791b",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" style=\"background-color:#ff6f69; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Prior Requirements</h2>\n",
    "</div>\n",
    "\n",
    "Make sure the followings are installed in Python environment\n",
    "\n",
    "Under Anaconda Prompt :\n",
    "- `Textblob` Package: `!pip install textblob`\n",
    "- `spacy` Package: `!pip install spacy`\n",
    "- Trained pipelines for English under Spacy: `python -m spacy download en`\n",
    "- Consolidated Text Preprocessing package: `!pip install git+ssh://git@github.com/HwaiTengTeoh/pt.git`\n",
    "- `emot` Package: `!pip install emot`\n",
    "- Download `Emoji_Dict.p` from download link: https://drive.google.com/open?id=1G1vIkkbqPBYPKHcQ8qy0G2zkoab2Qv4v\n",
    "- Download `Emoticon_Dict.p` from download link: https://drive.google.com/open?id=1HDpafp97gCl9xZTQWMgP2kKK_NuhENlE\n",
    "- `Gensim` Package: `!pip install gensim`\n",
    "- Spelling Check - `language-tool-python` Package: `!pip install language-tool-python` **(More precise)**\n",
    "- Contraction to Expansion - `pycontractions` Package: `!pip install pycontractions` **(More precise)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494aa141",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" style=\"background-color:#ff6f69; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Import Libraries/ Modules</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75d81dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Begin Python Imports\n",
    "import datetime, warnings, scipy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Progress bar\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# Text Cleaning & Normalization\n",
    "import re\n",
    "import pickle\n",
    "import spacy\n",
    "import nltk\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d2e0b",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" style=\"background-color:#ff6f69; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Import data</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6dc37a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>role</th>\n",
       "      <th>harmfulness_score</th>\n",
       "      <th>oth_language</th>\n",
       "      <th>file_index</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>digit_counts</th>\n",
       "      <th>uppercase_count</th>\n",
       "      <th>emails_count</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>exclaimation_count</th>\n",
       "      <th>questionmark_count</th>\n",
       "      <th>pos</th>\n",
       "      <th>pos_ADJ_counts</th>\n",
       "      <th>pos_ADP_counts</th>\n",
       "      <th>pos_ADV_counts</th>\n",
       "      <th>pos_AUX_counts</th>\n",
       "      <th>pos_CCONJ_counts</th>\n",
       "      <th>pos_DET_counts</th>\n",
       "      <th>pos_NOUN_counts</th>\n",
       "      <th>pos_INTJ_counts</th>\n",
       "      <th>pos_NUM_counts</th>\n",
       "      <th>pos_PART_counts</th>\n",
       "      <th>pos_PRON_counts</th>\n",
       "      <th>pos_PROPN_counts</th>\n",
       "      <th>pos_PUNCT_counts</th>\n",
       "      <th>pos_SCONJ_counts</th>\n",
       "      <th>pos_SYM_counts</th>\n",
       "      <th>pos_VERB_counts</th>\n",
       "      <th>pos_other_counts</th>\n",
       "      <th>ner</th>\n",
       "      <th>ner_CARDINAL_counts</th>\n",
       "      <th>ner_DATE_counts</th>\n",
       "      <th>ner_EVENT_counts</th>\n",
       "      <th>ner_FAC_counts</th>\n",
       "      <th>ner_GPE_counts</th>\n",
       "      <th>ner_LANGUAGE_counts</th>\n",
       "      <th>ner_LAW_counts</th>\n",
       "      <th>ner_LOC_counts</th>\n",
       "      <th>ner_MONEY_counts</th>\n",
       "      <th>ner_NORP_counts</th>\n",
       "      <th>ner_ORDINAL_counts</th>\n",
       "      <th>ner_ORG_counts</th>\n",
       "      <th>ner_PERCENT_counts</th>\n",
       "      <th>ner_PERSON_counts</th>\n",
       "      <th>ner_PRODUCT_counts</th>\n",
       "      <th>ner_QUANTITY_counts</th>\n",
       "      <th>ner_TIME_counts</th>\n",
       "      <th>ner_WORK_OF_ART_counts</th>\n",
       "      <th>text_check</th>\n",
       "      <th>emoji_counts</th>\n",
       "      <th>emoticon_counts</th>\n",
       "      <th>term_absolute_counts</th>\n",
       "      <th>term_allness_counts</th>\n",
       "      <th>term_badword_counts</th>\n",
       "      <th>term_negation_counts</th>\n",
       "      <th>term_diminisher_counts</th>\n",
       "      <th>term_intensifier_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s.0.w.0</td>\n",
       "      <td>Oh My God. :x</td>\n",
       "      <td>Non-Cyberbullying</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>xml_folder\\Askfm_conversation_10000_main.xml</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTJ PRON PROPN PUNCT PUNCT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oh god seal lip wear brace tongue tie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>s.0.w.0</td>\n",
       "      <td>opinion on Ross Golby?</td>\n",
       "      <td>Non-Cyberbullying</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>xml_folder\\Askfm_conversation_10002_main.xml</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NOUN ADP PROPN PROPN PUNCT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>opinion ross colby</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107104</th>\n",
       "      <td>107104</td>\n",
       "      <td>s.57.w.0</td>\n",
       "      <td>Like=15 likes</td>\n",
       "      <td>Non-Cyberbullying</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>xml_folder\\Askfm_conversation_9999_main.xml</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PROPN VERB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>like like</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107105</th>\n",
       "      <td>107105</td>\n",
       "      <td>s.58.w.0</td>\n",
       "      <td>no 5 likes for everyone</td>\n",
       "      <td>Non-Cyberbullying</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>xml_folder\\Askfm_conversation_9999_main.xml</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DET NUM NOUN ADP PRON</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>like</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       tag                     text              label  \\\n",
       "0                0   s.0.w.0            Oh My God. :x  Non-Cyberbullying   \n",
       "1                1   s.0.w.0   opinion on Ross Golby?  Non-Cyberbullying   \n",
       "107104      107104  s.57.w.0            Like=15 likes  Non-Cyberbullying   \n",
       "107105      107105  s.58.w.0  no 5 likes for everyone  Non-Cyberbullying   \n",
       "\n",
       "        role  harmfulness_score  oth_language  \\\n",
       "0       None                0.0             0   \n",
       "1       None                0.0             0   \n",
       "107104  None                0.0             0   \n",
       "107105  None                0.0             0   \n",
       "\n",
       "                                          file_index  word_count  char_count  \\\n",
       "0       xml_folder\\Askfm_conversation_10000_main.xml           4          10   \n",
       "1       xml_folder\\Askfm_conversation_10002_main.xml           4          19   \n",
       "107104   xml_folder\\Askfm_conversation_9999_main.xml           2          12   \n",
       "107105   xml_folder\\Askfm_conversation_9999_main.xml           5          19   \n",
       "\n",
       "        avg_word_len  stopword_count  hashtag_count  mention_count  \\\n",
       "0               2.50               0              0              0   \n",
       "1               4.75               1              0              0   \n",
       "107104          6.00               0              0              0   \n",
       "107105          3.80               3              0              0   \n",
       "\n",
       "        digit_counts  uppercase_count  emails_count  urls_count  punc_count  \\\n",
       "0                  0                0             0           0           2   \n",
       "1                  0                0             0           0           1   \n",
       "107104             0                0             0           0           1   \n",
       "107105             1                0             0           0           0   \n",
       "\n",
       "        exclaimation_count  questionmark_count                          pos  \\\n",
       "0                        0                   0  INTJ PRON PROPN PUNCT PUNCT   \n",
       "1                        0                   1   NOUN ADP PROPN PROPN PUNCT   \n",
       "107104                   0                   0                   PROPN VERB   \n",
       "107105                   0                   0        DET NUM NOUN ADP PRON   \n",
       "\n",
       "        pos_ADJ_counts  pos_ADP_counts  pos_ADV_counts  pos_AUX_counts  \\\n",
       "0                    0               0               0               0   \n",
       "1                    0               0               0               0   \n",
       "107104               0               0               0               0   \n",
       "107105               0               0               0               0   \n",
       "\n",
       "        pos_CCONJ_counts  pos_DET_counts  pos_NOUN_counts  pos_INTJ_counts  \\\n",
       "0                      0               0                0                0   \n",
       "1                      0               0                0                0   \n",
       "107104                 0               0                0                0   \n",
       "107105                 0               0                0                0   \n",
       "\n",
       "        pos_NUM_counts  pos_PART_counts  pos_PRON_counts  pos_PROPN_counts  \\\n",
       "0                    0                0                0                 0   \n",
       "1                    0                0                0                 0   \n",
       "107104               0                0                0                 0   \n",
       "107105               0                0                0                 0   \n",
       "\n",
       "        pos_PUNCT_counts  pos_SCONJ_counts  pos_SYM_counts  pos_VERB_counts  \\\n",
       "0                      0                 0               0                0   \n",
       "1                      0                 0               0                0   \n",
       "107104                 0                 0               0                0   \n",
       "107105                 0                 0               0                0   \n",
       "\n",
       "        pos_other_counts     ner  ner_CARDINAL_counts  ner_DATE_counts  \\\n",
       "0                      0     NaN                    0                0   \n",
       "1                      0  PERSON                    0                0   \n",
       "107104                 0  PERSON                    0                0   \n",
       "107105                 0     NaN                    0                0   \n",
       "\n",
       "        ner_EVENT_counts  ner_FAC_counts  ner_GPE_counts  ner_LANGUAGE_counts  \\\n",
       "0                      0               0               0                    0   \n",
       "1                      0               0               0                    0   \n",
       "107104                 0               0               0                    0   \n",
       "107105                 0               0               0                    0   \n",
       "\n",
       "        ner_LAW_counts  ner_LOC_counts  ner_MONEY_counts  ner_NORP_counts  \\\n",
       "0                    0               0                 0                0   \n",
       "1                    0               0                 0                0   \n",
       "107104               0               0                 0                0   \n",
       "107105               0               0                 0                0   \n",
       "\n",
       "        ner_ORDINAL_counts  ner_ORG_counts  ner_PERCENT_counts  \\\n",
       "0                        0               0                   0   \n",
       "1                        0               0                   0   \n",
       "107104                   0               0                   0   \n",
       "107105                   0               0                   0   \n",
       "\n",
       "        ner_PERSON_counts  ner_PRODUCT_counts  ner_QUANTITY_counts  \\\n",
       "0                       0                   0                    0   \n",
       "1                       0                   0                    0   \n",
       "107104                  0                   0                    0   \n",
       "107105                  0                   0                    0   \n",
       "\n",
       "        ner_TIME_counts  ner_WORK_OF_ART_counts  \\\n",
       "0                     0                       0   \n",
       "1                     0                       0   \n",
       "107104                0                       0   \n",
       "107105                0                       0   \n",
       "\n",
       "                                   text_check  emoji_counts  emoticon_counts  \\\n",
       "0       oh god seal lip wear brace tongue tie             0                0   \n",
       "1                          opinion ross colby             0                0   \n",
       "107104                              like like             0                0   \n",
       "107105                                   like             0                0   \n",
       "\n",
       "        term_absolute_counts  term_allness_counts  term_badword_counts  \\\n",
       "0                          0                    0                    1   \n",
       "1                          1                    1                    0   \n",
       "107104                     0                    0                    0   \n",
       "107105                     2                    2                    0   \n",
       "\n",
       "        term_negation_counts  term_diminisher_counts  term_intensifier_counts  \n",
       "0                          0                       0                        0  \n",
       "1                          0                       0                        0  \n",
       "107104                     0                       0                        0  \n",
       "107105                     1                       0                        0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read AMiCA data \n",
    "amica_data = pd.read_csv('amica_data_toclean_version.csv', encoding='utf8')\n",
    "\n",
    "# Check first 2 instances and last 2 instances\n",
    "amica_data.head(2).append(amica_data.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec33154",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" style=\"background-color:#ff6f69; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Initial Dataset Exploration</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5269c452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 107106 rows and 67 columns from the AMiCA dataset.\n"
     ]
    }
   ],
   "source": [
    "# Check dimension of dataset\n",
    "amica_data.shape\n",
    "print(\"There are \"+ str(amica_data.shape[0]) +\" rows and \"+ str(amica_data.shape[1]) +\" columns from the AMiCA dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd79acc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 107106 entries, 0 to 107105\n",
      "Data columns (total 67 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Unnamed: 0               107106 non-null  int64  \n",
      " 1   tag                      107106 non-null  object \n",
      " 2   text                     107106 non-null  object \n",
      " 3   label                    107106 non-null  object \n",
      " 4   role                     107106 non-null  object \n",
      " 5   harmfulness_score        107106 non-null  float64\n",
      " 6   oth_language             107106 non-null  int64  \n",
      " 7   file_index               107106 non-null  object \n",
      " 8   word_count               107106 non-null  int64  \n",
      " 9   char_count               107106 non-null  int64  \n",
      " 10  avg_word_len             107106 non-null  float64\n",
      " 11  stopword_count           107106 non-null  int64  \n",
      " 12  hashtag_count            107106 non-null  int64  \n",
      " 13  mention_count            107106 non-null  int64  \n",
      " 14  digit_counts             107106 non-null  int64  \n",
      " 15  uppercase_count          107106 non-null  int64  \n",
      " 16  emails_count             107106 non-null  int64  \n",
      " 17  urls_count               107106 non-null  int64  \n",
      " 18  punc_count               107106 non-null  int64  \n",
      " 19  exclaimation_count       107106 non-null  int64  \n",
      " 20  questionmark_count       107106 non-null  int64  \n",
      " 21  pos                      107106 non-null  object \n",
      " 22  pos_ADJ_counts           107106 non-null  int64  \n",
      " 23  pos_ADP_counts           107106 non-null  int64  \n",
      " 24  pos_ADV_counts           107106 non-null  int64  \n",
      " 25  pos_AUX_counts           107106 non-null  int64  \n",
      " 26  pos_CCONJ_counts         107106 non-null  int64  \n",
      " 27  pos_DET_counts           107106 non-null  int64  \n",
      " 28  pos_NOUN_counts          107106 non-null  int64  \n",
      " 29  pos_INTJ_counts          107106 non-null  int64  \n",
      " 30  pos_NUM_counts           107106 non-null  int64  \n",
      " 31  pos_PART_counts          107106 non-null  int64  \n",
      " 32  pos_PRON_counts          107106 non-null  int64  \n",
      " 33  pos_PROPN_counts         107106 non-null  int64  \n",
      " 34  pos_PUNCT_counts         107106 non-null  int64  \n",
      " 35  pos_SCONJ_counts         107106 non-null  int64  \n",
      " 36  pos_SYM_counts           107106 non-null  int64  \n",
      " 37  pos_VERB_counts          107106 non-null  int64  \n",
      " 38  pos_other_counts         107106 non-null  int64  \n",
      " 39  ner                      28519 non-null   object \n",
      " 40  ner_CARDINAL_counts      107106 non-null  int64  \n",
      " 41  ner_DATE_counts          107106 non-null  int64  \n",
      " 42  ner_EVENT_counts         107106 non-null  int64  \n",
      " 43  ner_FAC_counts           107106 non-null  int64  \n",
      " 44  ner_GPE_counts           107106 non-null  int64  \n",
      " 45  ner_LANGUAGE_counts      107106 non-null  int64  \n",
      " 46  ner_LAW_counts           107106 non-null  int64  \n",
      " 47  ner_LOC_counts           107106 non-null  int64  \n",
      " 48  ner_MONEY_counts         107106 non-null  int64  \n",
      " 49  ner_NORP_counts          107106 non-null  int64  \n",
      " 50  ner_ORDINAL_counts       107106 non-null  int64  \n",
      " 51  ner_ORG_counts           107106 non-null  int64  \n",
      " 52  ner_PERCENT_counts       107106 non-null  int64  \n",
      " 53  ner_PERSON_counts        107106 non-null  int64  \n",
      " 54  ner_PRODUCT_counts       107106 non-null  int64  \n",
      " 55  ner_QUANTITY_counts      107106 non-null  int64  \n",
      " 56  ner_TIME_counts          107106 non-null  int64  \n",
      " 57  ner_WORK_OF_ART_counts   107106 non-null  int64  \n",
      " 58  text_check               107101 non-null  object \n",
      " 59  emoji_counts             107106 non-null  int64  \n",
      " 60  emoticon_counts          107106 non-null  int64  \n",
      " 61  term_absolute_counts     107106 non-null  int64  \n",
      " 62  term_allness_counts      107106 non-null  int64  \n",
      " 63  term_badword_counts      107106 non-null  int64  \n",
      " 64  term_negation_counts     107106 non-null  int64  \n",
      " 65  term_diminisher_counts   107106 non-null  int64  \n",
      " 66  term_intensifier_counts  107106 non-null  int64  \n",
      "dtypes: float64(2), int64(57), object(8)\n",
      "memory usage: 54.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check column type\n",
    "amica_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f4b9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Unwanted column\n",
    "amica_data.drop('Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9929be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Unwanted column\n",
    "amica_data=amica_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14006f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 107106 entries, 0 to 107105\n",
      "Data columns (total 66 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   tag                      107106 non-null  object \n",
      " 1   text                     107106 non-null  object \n",
      " 2   label                    107106 non-null  object \n",
      " 3   role                     107106 non-null  object \n",
      " 4   harmfulness_score        107106 non-null  float64\n",
      " 5   oth_language             107106 non-null  int64  \n",
      " 6   file_index               107106 non-null  object \n",
      " 7   word_count               107106 non-null  int64  \n",
      " 8   char_count               107106 non-null  int64  \n",
      " 9   avg_word_len             107106 non-null  float64\n",
      " 10  stopword_count           107106 non-null  int64  \n",
      " 11  hashtag_count            107106 non-null  int64  \n",
      " 12  mention_count            107106 non-null  int64  \n",
      " 13  digit_counts             107106 non-null  int64  \n",
      " 14  uppercase_count          107106 non-null  int64  \n",
      " 15  emails_count             107106 non-null  int64  \n",
      " 16  urls_count               107106 non-null  int64  \n",
      " 17  punc_count               107106 non-null  int64  \n",
      " 18  exclaimation_count       107106 non-null  int64  \n",
      " 19  questionmark_count       107106 non-null  int64  \n",
      " 20  pos                      107106 non-null  object \n",
      " 21  pos_ADJ_counts           107106 non-null  int64  \n",
      " 22  pos_ADP_counts           107106 non-null  int64  \n",
      " 23  pos_ADV_counts           107106 non-null  int64  \n",
      " 24  pos_AUX_counts           107106 non-null  int64  \n",
      " 25  pos_CCONJ_counts         107106 non-null  int64  \n",
      " 26  pos_DET_counts           107106 non-null  int64  \n",
      " 27  pos_NOUN_counts          107106 non-null  int64  \n",
      " 28  pos_INTJ_counts          107106 non-null  int64  \n",
      " 29  pos_NUM_counts           107106 non-null  int64  \n",
      " 30  pos_PART_counts          107106 non-null  int64  \n",
      " 31  pos_PRON_counts          107106 non-null  int64  \n",
      " 32  pos_PROPN_counts         107106 non-null  int64  \n",
      " 33  pos_PUNCT_counts         107106 non-null  int64  \n",
      " 34  pos_SCONJ_counts         107106 non-null  int64  \n",
      " 35  pos_SYM_counts           107106 non-null  int64  \n",
      " 36  pos_VERB_counts          107106 non-null  int64  \n",
      " 37  pos_other_counts         107106 non-null  int64  \n",
      " 38  ner                      28519 non-null   object \n",
      " 39  ner_CARDINAL_counts      107106 non-null  int64  \n",
      " 40  ner_DATE_counts          107106 non-null  int64  \n",
      " 41  ner_EVENT_counts         107106 non-null  int64  \n",
      " 42  ner_FAC_counts           107106 non-null  int64  \n",
      " 43  ner_GPE_counts           107106 non-null  int64  \n",
      " 44  ner_LANGUAGE_counts      107106 non-null  int64  \n",
      " 45  ner_LAW_counts           107106 non-null  int64  \n",
      " 46  ner_LOC_counts           107106 non-null  int64  \n",
      " 47  ner_MONEY_counts         107106 non-null  int64  \n",
      " 48  ner_NORP_counts          107106 non-null  int64  \n",
      " 49  ner_ORDINAL_counts       107106 non-null  int64  \n",
      " 50  ner_ORG_counts           107106 non-null  int64  \n",
      " 51  ner_PERCENT_counts       107106 non-null  int64  \n",
      " 52  ner_PERSON_counts        107106 non-null  int64  \n",
      " 53  ner_PRODUCT_counts       107106 non-null  int64  \n",
      " 54  ner_QUANTITY_counts      107106 non-null  int64  \n",
      " 55  ner_TIME_counts          107106 non-null  int64  \n",
      " 56  ner_WORK_OF_ART_counts   107106 non-null  int64  \n",
      " 57  text_check               107101 non-null  object \n",
      " 58  emoji_counts             107106 non-null  int64  \n",
      " 59  emoticon_counts          107106 non-null  int64  \n",
      " 60  term_absolute_counts     107106 non-null  int64  \n",
      " 61  term_allness_counts      107106 non-null  int64  \n",
      " 62  term_badword_counts      107106 non-null  int64  \n",
      " 63  term_negation_counts     107106 non-null  int64  \n",
      " 64  term_diminisher_counts   107106 non-null  int64  \n",
      " 65  term_intensifier_counts  107106 non-null  int64  \n",
      "dtypes: float64(2), int64(56), object(8)\n",
      "memory usage: 53.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Last check column type\n",
    "amica_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acfaf915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                             Oh My God. :x\n",
       "1                                    opinion on Ross Golby?\n",
       "2         dont know him, I just see him walking past jub...\n",
       "3                                                 Dick size\n",
       "4                                           you should know\n",
       "                                ...                        \n",
       "107101    Do you believe that playing is more important ...\n",
       "107102                                     yeah I am with u\n",
       "107103         I already ans that question just scroll down\n",
       "107104                                        Like=15 likes\n",
       "107105                              no 5 likes for everyone\n",
       "Name: text, Length: 107106, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last check column type\n",
    "amica_data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c2c4c",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" style=\"background-color:#ff6f69; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>(Ignore) Handle of missing data - None</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "170c35ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of missing data in columns\n",
      "  column_name  percentage\n",
      "0         ner   73.373107\n",
      "1  text_check    0.004668\n"
     ]
    }
   ],
   "source": [
    "# Calculate the proportion of missing data\n",
    "\n",
    "def checkMissing(data,perc=0):\n",
    "    \"\"\" \n",
    "    Function that takes in a dataframe and returns\n",
    "    the percentage of missing value.\n",
    "    \"\"\"\n",
    "    missing = [(i, data[i].isna().mean()*100) for i in data]\n",
    "    missing = pd.DataFrame(missing, columns=[\"column_name\", \"percentage\"])\n",
    "    missing = missing[missing.percentage > perc]\n",
    "    print(missing.sort_values(\"percentage\", ascending=False).reset_index(drop=True))\n",
    "\n",
    "print(\"Proportion of missing data in columns\")\n",
    "checkMissing(amica_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b61d3a",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" style=\"background-color:#ff6f69; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Text Preprocessing Pipeline </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbc2bba5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycontractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25808/2334536606.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocess_text\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlanguage_tool_python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpycontractions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontractions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mContractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycontractions'"
     ]
    }
   ],
   "source": [
    "import preprocess_text as pt\n",
    "import language_tool_python\n",
    "from pycontractions.contractions import Contractions\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "\n",
    "# Functions\n",
    "def get_term_list(path):\n",
    "    '''\n",
    "    Function to import term list file\n",
    "    '''\n",
    "    word_list = []\n",
    "    with open(path,\"r\") as f:\n",
    "        for line in f:\n",
    "            word = line.replace(\"\\n\",\"\").strip()\n",
    "            word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def get_vocab(corpus):\n",
    "    '''\n",
    "    Function returns unique words in document corpus\n",
    "    '''\n",
    "    # vocab set\n",
    "    unique_words = set()\n",
    "    \n",
    "    # looping through each document in corpus\n",
    "    for document in tqdm(corpus):\n",
    "        for word in document.split(\" \"):\n",
    "            if len(word) > 2:\n",
    "                unique_words.add(word)\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "def create_profane_mapping(profane_words,vocabulary):\n",
    "    '''\n",
    "    Function creates a mapping between commonly found profane words and words in \n",
    "    document corpus \n",
    "    '''\n",
    "    \n",
    "    # mapping dictionary\n",
    "    mapping_dict = dict()\n",
    "    \n",
    "    # looping through each profane word\n",
    "    for profane in tqdm(profane_words):\n",
    "        mapped_words = set()\n",
    "        \n",
    "        # looping through each word in vocab\n",
    "        for word in vocabulary:\n",
    "            # mapping only if ratio > 80\n",
    "            try:\n",
    "                if fuzz.ratio(profane,word) > 90:\n",
    "                    mapped_words.add(word)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # list of all vocab words for given profane word\n",
    "        mapping_dict[profane] = mapped_words\n",
    "    \n",
    "    return mapping_dict\n",
    "\n",
    "def replace_words(corpus,mapping_dict):\n",
    "    '''\n",
    "    Function replaces obfuscated profane words using a mapping dictionary\n",
    "    '''\n",
    "    \n",
    "    processed_corpus = []\n",
    "    \n",
    "    # iterating over each document in the corpus\n",
    "    for document in tqdm(corpus):\n",
    "        \n",
    "        # splitting sentence to word\n",
    "        comment = document.split()\n",
    "        \n",
    "        # iterating over mapping_dict\n",
    "        for mapped_word,v in mapping_dict.items():\n",
    "            \n",
    "            # comparing target word to each comment word \n",
    "            for target_word in v:\n",
    "                \n",
    "                # each word in comment\n",
    "                for i,word in enumerate(comment):\n",
    "                    if word == target_word:\n",
    "                        comment[i] = mapped_word\n",
    "        \n",
    "        # joining comment words\n",
    "        document = \" \".join(comment)\n",
    "        document = document.strip()\n",
    "                    \n",
    "        processed_corpus.append(document)\n",
    "        \n",
    "    return processed_corpus\n",
    "\n",
    "# Counts of term by category\n",
    "countvec = CountVectorizer(ngram_range=(1,3))\n",
    "def get_term_counts(x,category):\n",
    "    \n",
    "    # Split input text by unigram, bigram and trigram\n",
    "    # as the keywords may span up to 3 words\n",
    "    def get_ngram_text(x):\n",
    "        \n",
    "        try:\n",
    "            countvec.fit_transform(x)\n",
    "            text_list = countvec.get_feature_names()\n",
    "            return text_list\n",
    "\n",
    "        except ValueError:\n",
    "            return [' '] # to handle scenario where text input are all stop words only\n",
    "    \n",
    "    # check the existence of word by category\n",
    "    term_category = [t for t in get_ngram_text(x) if t in category]\n",
    "    \n",
    "    # return the number of occurence\n",
    "    return len(term_category)\n",
    "\n",
    "\n",
    "# Import external list, store as list\n",
    "term_absolute_list = get_term_list(\"term_list/compiled_absolute.txt\")\n",
    "term_allness_list = get_term_list(\"term_list/compiled_allness.txt\")\n",
    "term_badword_list = get_term_list(\"term_list/compiled_badword.txt\")\n",
    "term_negation_list = get_term_list(\"term_list/compiled_negation.txt\")\n",
    "term_diminisher_list = get_term_list(\"term_list/compiled_diminisher.txt\")\n",
    "term_intensifier_list = get_term_list(\"term_list/compiled_intensifier.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cbd4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Text Preprocessing Pipeline #\n",
    "###############################\n",
    "\n",
    "def text_preprocessing_pipeline(df=amica_data,\n",
    "                                textual_statistics=False,\n",
    "                                remove_url=False,\n",
    "                                remove_email=False,\n",
    "                                remove_user_mention=False,\n",
    "                                remove_html=False,\n",
    "                                remove_space_single_char=False,\n",
    "                                normalize_elongated_char=False,\n",
    "                                normalize_emoji=False,\n",
    "                                normalize_emoticon=False,\n",
    "                                normalize_accented=False,\n",
    "                                lower_case=False,\n",
    "                                normalize_slang=False,\n",
    "                                normalize_badterm=False,\n",
    "                                spelling_check=False,\n",
    "                                normalize_contraction=False,\n",
    "                                term_list=False,\n",
    "                                remove_numeric=False,\n",
    "                                remove_stopword=False,\n",
    "                                keep_pronoun=False,\n",
    "                                remove_punctuation=False,\n",
    "                                pos=False,\n",
    "                                ner=False,\n",
    "                                lemmatise=False\n",
    "                               ):\n",
    "    '''\n",
    "    -------------\n",
    "     Description\n",
    "    -------------\n",
    "    Function that compile all preprocessing steps in one go\n",
    "    \n",
    "    -----------\n",
    "     Parameter\n",
    "    -----------\n",
    "    df: Data Frame\n",
    "    textual_statistics: Boolean\n",
    "    remove_url: Boolean\n",
    "    remove_email: Boolean\n",
    "    remove_user_mention: Boolean\n",
    "    remove_html: Boolean\n",
    "    remove_space_single_char: Boolean\n",
    "    normalize_elongated_char: Boolean\n",
    "    normalize_emoji: Boolean\n",
    "    normalize_emoticon: Boolean\n",
    "    normalize_accented: Boolean\n",
    "    lower_case: Boolean\n",
    "    normalize_slang: Boolean\n",
    "    normalize_badterm: Boolean\n",
    "    spelling_check: Boolean\n",
    "    normalize_contraction: Boolean\n",
    "    remove_numeric: Boolean\n",
    "    remove_stopword: Boolean\n",
    "    keep_pronoun: Boolean\n",
    "    remove_punctuation: Boolean\n",
    "    pos: Boolean\n",
    "    ner: Boolean\n",
    "    lemmatise: Boolean\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if textual_statistics:\n",
    "        print('Developing textual statistics from original text')\n",
    "        df['word_count'] = df['text'].progress_apply(lambda x: pt.get_wordcounts(x))\n",
    "        df['char_count'] = df['text'].progress_apply(lambda x: pt.get_char_counts(x))\n",
    "        df['avg_word_len'] = df['text'].progress_apply(lambda x: pt.get_avg_wordlength(x))\n",
    "        df['stopword_count'] = df['text'].progress_apply(lambda x: pt.get_stopwords_counts(x))\n",
    "        df['hashtag_count'] = df['text'].progress_apply(lambda x: pt.get_hashtag_counts(x))\n",
    "        df['mention_count'] = df['text'].progress_apply(lambda x: pt.get_mention_counts(x))\n",
    "        df['digit_counts'] = df['text'].progress_apply(lambda x: pt.get_digit_counts(x))\n",
    "        df['uppercase_count'] = df['text'].progress_apply(lambda x: pt.get_uppercase_counts(x))\n",
    "        df['emails_count'] = df['text'].progress_apply(lambda x: pt.get_emails(x))\n",
    "        df['urls_count'] = df['text'].progress_apply(lambda x: pt.get_urls(x))\n",
    "        df['punc_count'] = df['text'].progress_apply(lambda x: pt.get_punc_counts(x))\n",
    "        df[\"exclaimation_count\"] = df[\"text\"].progress_apply(lambda x: x.count(\"!\"))\n",
    "        df[\"questionmark_count\"] = df[\"text\"].progress_apply(lambda x: x.count(\"?\"))\n",
    "    \n",
    "    if pos:\n",
    "        print('Text Preprocessing: Developing POS tag count')\n",
    "        df[\"pos\"] = df[\"text\"].progress_apply(lambda x: pt.get_pos_tag(x))\n",
    "        df[\"pos_ADJ_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"ADJ\"))     #adjective\n",
    "        df[\"pos_ADP_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"ADP\"))     #adposition\n",
    "        df[\"pos_ADV_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"ADV\"))     #adverb\n",
    "        df[\"pos_AUX_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"AUX\"))     #auxiliary\n",
    "        df[\"pos_CCONJ_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"CCONJ\")) #coordinating conjunction\n",
    "        df[\"pos_DET_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"DET\"))     #determiner\n",
    "        df[\"pos_NOUN_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"NOUN\"))   #noun\n",
    "        df[\"pos_INTJ_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"INTJ\"))   #interjection\n",
    "        df[\"pos_NUM_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"NUM\"))     #numeral\n",
    "        df[\"pos_PART_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"PART\"))   #particle\n",
    "        df[\"pos_PRON_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"PRON\"))   #pronoun\n",
    "        df[\"pos_PROPN_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"PROPN\")) #proper noun\n",
    "        df[\"pos_PUNCT_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"PUNCT\")) #punctuation\n",
    "        df[\"pos_SCONJ_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"SCONJ\")) #subordinating conjunction\n",
    "        df[\"pos_SYM_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"SYM\"))     #symbol\n",
    "        df[\"pos_VERB_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"VERB\"))   #verb\n",
    "        df[\"pos_other_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"X\"))     #other\n",
    "    \n",
    "    if ner:\n",
    "        print('Text Preprocessing: Developing NER tag count')\n",
    "        df[\"ner\"] = df[\"text\"].progress_apply(lambda x: pt.get_ner(x))\n",
    "        ner_lst = nlp.pipe_labels['ner']\n",
    "        for ner in ner_lst:\n",
    "             df[\"ner_\"+ ner +\"_counts\"] =  df[\"ner\"].apply(lambda x: pt.get_ner_counts(x,ner))\n",
    "                \n",
    "    if remove_url:\n",
    "        print('Text Preprocessing: Remove URL')\n",
    "        df['text_check'] = df['text'].progress_apply(lambda x: pt.remove_urls(x))\n",
    "        \n",
    "    if remove_email:\n",
    "        print('Text Preprocessing: Remove email')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_emails(x))\n",
    "        \n",
    "    if remove_user_mention:\n",
    "        print('Text Preprocessing: Remove user mention')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_mention(x))\n",
    "    \n",
    "    if remove_html:\n",
    "        print('Text Preprocessing: Remove html element')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_html_tags(x))\n",
    "        \n",
    "    if remove_space_single_char:\n",
    "        print('Text Preprocessing: Remove single spcae between single characters e.g F U C K')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_space_single_chars(x))\n",
    "        \n",
    "    if normalize_elongated_char:\n",
    "        print('Text Preprocessing: Reduction of elongated characters')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_elongated_chars(x))\n",
    "        \n",
    "    if normalize_emoji:\n",
    "        print('Text Preprocessing: Normalize and count emoji')\n",
    "        df['emoji_counts'] = df['text_check'].progress_apply(lambda x: pt.get_emoji_counts(x))\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.convert_emojis(x))\n",
    "        \n",
    "        \n",
    "    if normalize_emoticon:\n",
    "        print('Text Preprocessing: Normalize and count emoticon')\n",
    "        df['emoticon_counts'] = df['text_check'].progress_apply(lambda x: pt.get_emoticon_counts(x))\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.convert_emoticons(x))\n",
    "        \n",
    "        \n",
    "    if normalize_accented:\n",
    "        print('Text Preprocessing: Normalize accented character')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_accented_chars(x))\n",
    "        \n",
    "    if lower_case:\n",
    "        print('Text Preprocessing: Convert to lower case')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: str(x).lower())\n",
    "    \n",
    "    if normalize_slang:\n",
    "        print('Text Preprocessing: Normalize slang')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.slang_resolution(x))\n",
    "        \n",
    "    if normalize_badterm:\n",
    "        print('Text Preprocessing: Replace obfuscated bad term')\n",
    "        # unique words in vocab \n",
    "        unique_words = get_vocab(corpus= df['text_check'])\n",
    "        \n",
    "        # creating mapping dict \n",
    "        mapping_dict = create_profane_mapping(profane_words=term_badword_list,vocabulary=unique_words)\n",
    "        \n",
    "        df['text_check'] = replace_words(corpus=df['text_check'],\n",
    "                                                 mapping_dict=mapping_dict)\n",
    "        \n",
    "    if spelling_check:\n",
    "        print('Text Preprocessing: Spelling Check')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: tool.correct(x))\n",
    "        tool.close()\n",
    "        \n",
    "    if normalize_contraction:\n",
    "        print('Text Preprocessing: Contraction to Expansion')\n",
    "        \n",
    "        # Special handling to prevent code from taking forever to run\n",
    "        hardcode_clean_50702 = df['text_check'].iloc[50702].replace(\"'d\",\" would\").replace(\"wasn't\",\"was not\").replace(\"wouldn't\",\"would not\").replace(\"'s\",\" is\").replace(\"'m\",\" am\")\n",
    "        df['text_check'].iloc[50702] = hardcode_clean_50702\n",
    "\n",
    "        hardcode_clean_107720 = df['text_check'].iloc[107720].replace(\"'d\",\" would\").replace(\"wasn't\",\"was not\").replace(\"wouldn't\",\"would not\")\n",
    "        df['text_check'].iloc[107720] = hardcode_clean_107720\n",
    "\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: ''.join(list(cont.expand_texts([x], precise=True))))\n",
    "    \n",
    "    if term_list:\n",
    "        print('Developing Binary Features for existence of terms by category')\n",
    "        df['term_absolute_counts'] = df['text_check'].progress_apply(lambda x: get_term_counts([x],category=term_absolute_list))\n",
    "        df['term_allness_counts'] = df['text_check'].progress_apply(lambda x: get_term_counts([x],category=term_allness_list))\n",
    "        df['term_badword_counts'] = df['text_check'].progress_apply(lambda x: get_term_counts([x],category=term_badword_list))\n",
    "        df['term_negation_counts'] = df['text_check'].progress_apply(lambda x: get_term_counts([x],category=term_negation_list))\n",
    "        df['term_diminisher_counts'] = df['text_check'].progress_apply(lambda x: get_term_counts([x],category=term_diminisher_list))\n",
    "        df['term_intensifier_counts'] = df['text_check'].progress_apply(lambda x: get_term_counts([x],category=term_intensifier_list))\n",
    "\n",
    "    if remove_numeric: \n",
    "        print('Text Preprocessing: Remove numeric')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_numeric(x))\n",
    "        \n",
    "    if remove_punctuation:\n",
    "        print('Text Preprocessing: Remove punctuations')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_special_chars(x))\n",
    "        \n",
    "    if remove_stopword:\n",
    "        print('Text Preprocessing: Remove stopword')\n",
    "        if keep_pronoun:\n",
    "            print('Text Preprocessing: and, keep Pronoun')\n",
    "        df[\"text_check\"] = df[\"text_check\"].progress_apply(lambda x: pt.remove_stopwords(x,keep_pronoun=keep_pronoun))\n",
    "        \n",
    "    # Remove multiple spaces\n",
    "    print('Text Preprocessing: Remove multiple spaces')\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: ' '.join(x.split()))\n",
    "    \n",
    "    if lemmatise:\n",
    "        print('Text Preprocessing: Lemmatization')\n",
    "        df[\"text_check\"] = df[\"text_check\"].progress_apply(lambda x: pt.make_base(x))\n",
    "        \n",
    "    # Make sure remove multiple spaces\n",
    "    # df['text_check'] = df['text_check'].progress_apply(lambda x: ' '.join(x.split()))\n",
    "    \n",
    "    # Make sure lower case for all again\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: str(x).lower())\n",
    "    \n",
    "    # Remove empty text after cleaning\n",
    "    print('Last Step: Remove empty text after preprocessing. Done')\n",
    "    df = df[~df['text_check'].isna()]\n",
    "    df = df[df['text_check'] != '']\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef264f",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" style=\"background-color:#ff6f69; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Inspect Cleaning Process</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052c643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3183c661",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" style=\"background-color:#ff6f69; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>Output: Preprocessed and Cleaned Data</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82205cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing textual statistics from original text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d27f1669df4482b1da03ae421f69d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2996435e770b49728bfd5395bb5087e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee60022965f4c00a7e46814a50dbff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6bd9d0a9ff4d18b8d04bb58114cb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ef3b8631494fec96d2a24ff61f3262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f7c625a59146cf9de9f843792da74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e66a148e804a52b800b9f9c78d33bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf928763a65d4aabb823a96f68b8b67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc34f843bb9467aad7896b57f881972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35acf1c5ec494da1beccba66c7794541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19657a368a7e4e57b7daf891cffda720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c2d8052aec45d385a435dd85bbd0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e599e2005eb74ed882adca6fd893e04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Developing POS tag count\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8550ed40da48b596b7fad722faebbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb95bb748dc14daca8803cdfd92d1d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18969313f6d848849a9c13b46069bedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac7a85b94d845d6b230ce9e94f89d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dc441c44ef4452ab7fc8e63a125668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e659d12d284496e9ed5063c8439e8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d18947fbbd4333a1b4fcaa49bcd961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1db6b7ae404438ab380256eeb64ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7562b7c98d4e92a72b3e1ee7aec13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4095bd1131434bceb8675b1ca78ccc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dd770ab4b74eebb81e2752daa67129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8a6a467d2c410ba4e701e1cba40367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70448b2a98d64e46853c3a75516b5a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64126248cd4f4ed0aac55bbb4aed4200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486091a62b3b49e0b8a026dabd5f5ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281654760a5d44c7ba54b9647e2662f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9b636d837842ac94067691ec31466e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b974082b1bb4fd6b63d6a080416491f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Developing NER tag count\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ebb4dfa1404bf58f4a56d842f694c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove URL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c59d0bbfae4cec9fe14354092ab627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove email\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174cdc3fb8c2490aa081f91eb45a828c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove user mention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9194ebc43d945f2b15be5fdd486f155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove html element\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b8d64996094b0a8513780f61ec510e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove single spcae between single characters e.g F U C K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3b4ae46c024ceaa68e41c7f26882ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Reduction of elongated characters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac56ba52abee485b8563ff5a44793bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Normalize and count emoji\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00530f9d93b489d93680e08ccdfc132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7927286a14174964a6dbcdf646ca387b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Normalize and count emoticon\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbe6e4e0e194a5d9319c67c9c0e6174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7222ce6205e49b9b51b0e56a52580db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Normalize accented character\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e246ffed0bf4298ac963f6ab16b90bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Convert to lower case\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c0573aa5f8441f8ecea25ac1bd1957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Normalize slang\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aab0971e5524d628032cd3f3e3da1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Replace obfuscated bad term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 113694/113694 [00:00<00:00, 541277.61it/s]\n",
      "100%|| 1921/1921 [03:45<00:00,  8.53it/s]\n",
      "100%|| 113694/113694 [01:21<00:00, 1394.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Spelling Check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43ba75fb0da4165a3152fac8c67a92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Contraction to Expansion\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc47343ca1774b159fa26f7cb63c3320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n",
      "At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing Binary Features for existence of terms by category\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1267841eff4e62b074c64f0cba99c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4abbf8fbe2d4873ae91eb848c51596a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cc0ef7e91a48e59c6fdcc35a4da1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3a32725ef143288ba3b6835d60c99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc9c7bebdea4f1685c0a048d5ce3cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6794ac10e4294fa58b6874f5ac758811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove numeric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69115a9f24e248b09232e2ac302b26b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove punctuations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241a09ef96cc45ba86d6bfd526d35f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove multiple spaces\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df2b2f225594f1990623dc37c705aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Lemmatization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f140ddf3b2b4277a1bf62b9839332cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f2d869f0434a658a9ade1040007061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Step: Remove empty text after preprocessing. Done\n"
     ]
    }
   ],
   "source": [
    "amica_data_clean_with_stopword = text_preprocessing_pipeline(\n",
    "                                    df=amica_data,\n",
    "                                    textual_statistics=True,\n",
    "                                    remove_url=True,\n",
    "                                    remove_email=True,\n",
    "                                    remove_user_mention=True,\n",
    "                                    remove_html=True,\n",
    "                                    remove_space_single_char=True,\n",
    "                                    normalize_elongated_char=True,\n",
    "                                    normalize_emoji=True,\n",
    "                                    normalize_emoticon=True,\n",
    "                                    normalize_accented=True,\n",
    "                                    lower_case=True,\n",
    "                                    normalize_slang=True,\n",
    "                                    normalize_badterm=True,\n",
    "                                    spelling_check=True,\n",
    "                                    normalize_contraction=True,\n",
    "                                    term_list=True,\n",
    "                                    remove_numeric=True,\n",
    "                                    remove_stopword=False, # Keep stopwords\n",
    "                                    keep_pronoun=False,  # Keep pronoun\n",
    "                                    remove_punctuation=True,\n",
    "                                    pos=True,\n",
    "                                    ner=True,\n",
    "                                    lemmatise=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19470ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "amica_data_clean_with_stopword_base1 =  amica_data_clean_with_stopword.copy()\n",
    "amica_data_clean_with_stopword_base2 =  amica_data_clean_with_stopword.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7be718eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove stopword\n",
      "Text Preprocessing: and, keep Pronoun\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01103e6338484e8992fbd25707019eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove multiple spaces\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95028f99a24b4693bd8ba8d9f504318e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206df1a3fb254d51b7f7923d5f077b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Step: Remove empty text after preprocessing. Done\n"
     ]
    }
   ],
   "source": [
    "amica_data_clean_no_stopword_pronoun = text_preprocessing_pipeline(\n",
    "                                            df=amica_data_clean_with_stopword_base1,\n",
    "                                            remove_stopword=True, # Remove stopwords\n",
    "                                            keep_pronoun=True) # But keep pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc007383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove stopword\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5487b866e34e4adca19a6245e7f456b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove multiple spaces\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b1933388944c18a60f08b7cc39283b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dde768bb674ff88f89ac0aa2c5416d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Step: Remove empty text after preprocessing. Done\n"
     ]
    }
   ],
   "source": [
    "amica_data_clean_no_stopword_all = text_preprocessing_pipeline(\n",
    "                                        df=amica_data_clean_with_stopword_base2,\n",
    "                                        remove_stopword=True, # Remove all stopwords\n",
    "                                        keep_pronoun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab397fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "amica_data_clean_with_stopword.to_csv('amica_data_clean_with_stopword.csv')\n",
    "amica_data_clean_no_stopword_pronoun.to_csv('amica_data_clean_no_stopword_pronoun.csv')\n",
    "amica_data_clean_no_stopword_all.to_csv('amica_data_clean_no_stopword_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2234fe",
   "metadata": {},
   "source": [
    "<h1><center>- END Preprocessing and Cleaning -</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "357743f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68cf981d13c4718b70c484659e52394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1067429"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick count on number of words\n",
    "amica_data_clean_with_stopword['wc'] = amica_data_clean_with_stopword['text_check'].progress_apply(lambda x: pt.get_wordcounts(x))\n",
    "amica_data_clean_with_stopword['wc'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28828528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            oh my god seal lip or wear brace or tongue tie\n",
       "1                                     opinion on ross colby\n",
       "2         do not know him i just see him walk past jubil...\n",
       "3                                                 dick size\n",
       "4                                           you should know\n",
       "                                ...                        \n",
       "112244                                   yeah i am with you\n",
       "112245                                   where are you from\n",
       "112246         i already and that question just scroll down\n",
       "112247                                            like like\n",
       "112248                                 no like for everyone\n",
       "Name: text_check, Length: 112249, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amica_data_clean_with_stopword['text_check']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "178f930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1bf3e97ac443859a47e56789e395c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "659654"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick count on number of words\n",
    "amica_data_clean_no_stopword_pronoun['wc'] = amica_data_clean_no_stopword_pronoun['text_check'].progress_apply(lambda x: pt.get_wordcounts(x))\n",
    "amica_data_clean_no_stopword_pronoun['wc'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c8a333d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         oh god seal lip wear brace tongue tie\n",
       "1                            opinion ross colby\n",
       "2           know him i him walk past jubilee no\n",
       "3                                     dick size\n",
       "4                                      you know\n",
       "                          ...                  \n",
       "109935                               yeah i you\n",
       "109936                                      you\n",
       "109937                        i question scroll\n",
       "109938                                like like\n",
       "109939                            like everyone\n",
       "Name: text_check, Length: 109940, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amica_data_clean_no_stopword_pronoun['text_check']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29044b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739f8f0d9002499ea6d6c37c8a9eb9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "463837"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick count on number of words\n",
    "amica_data_clean_no_stopword_all['wc'] = amica_data_clean_no_stopword_all['text_check'].progress_apply(lambda x: pt.get_wordcounts(x))\n",
    "amica_data_clean_no_stopword_all['wc'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e317db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         oh god seal lip wear brace tongue tie\n",
       "1                            opinion ross colby\n",
       "2                        know walk past jubilee\n",
       "3                                     dick size\n",
       "4                                          know\n",
       "                          ...                  \n",
       "107101            believe playing important win\n",
       "107102                                     yeah\n",
       "107103                          question scroll\n",
       "107104                                like like\n",
       "107105                                     like\n",
       "Name: text_check, Length: 107106, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amica_data_clean_no_stopword_all['text_check']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
